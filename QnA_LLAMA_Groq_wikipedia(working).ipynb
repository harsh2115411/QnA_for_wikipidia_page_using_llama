{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jXflsEpWoyb",
        "outputId": "97130d7d-2aa4-4b1d-938d-2f8b1c471f41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: youtube-transcript-api in /usr/local/lib/python3.10/dist-packages (0.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.10/dist-packages (3.7.4.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from youtube-transcript-api) (0.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.12.14)\n"
          ]
        }
      ],
      "source": [
        "pip install beautifulsoup4 youtube-transcript-api requests typing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3B9SLUNKW7Af"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import logging\n",
        "from typing import List, Optional\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from youtube_transcript_api import YouTubeTranscriptApi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4ag7bTmWcH8",
        "outputId": "947734ea-c914-40bb-83f9-20aa84adc4d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: Photosynthesis is a process in which green plants make their own food from sunlight through the help of leaves to produce sugar. It is the process by which these organisms convert light energy into chemical energy, which is used to produce food. The process of photosynthesis occurs in the chloroplasts in leaves (or other green tissues) and involves two main sets of reactions: light-dependent reactions and light-independent reactions.\n",
            "\n",
            "Here's a step-by-step explanation of the process:\n",
            "\n",
            "1. **Light-Dependent Reactions**: Light energy from the sun hits the chloroplasts in the plant, causing an enzyme to split water molecules (photolysis). This results in the production of oxygen, hydrogen, and electrons.\n",
            "\n",
            "2. **Production of ATP and NADPH**: The sunlight energized electrons convert NADP into NADPH, which is then used in the light-independent reactions. Oxygen gas diffuses out of the plant as a waste product of photosynthesis, and ATP is synthesized from ADP and inorganic phosphate.\n",
            "\n",
            "3. **Light-Independent Reactions (Calvin Cycle)**: The products of the light-dependent reactions (ATP and NADPH) are used to build up sugars using carbon dioxide and various other chemicals found in the plant. This process occurs in the grana of chloroplasts.\n",
            "\n",
            "4. **Production of Glucose**: Carbon dioxide diffuses into the plant and is combined with ATP, NADPH, and various other chemicals to form glucose.\n",
            "\n",
            "5. **Transportation of Glucose**: The produced glucose is transported around the plant by translocation.\n",
            "\n",
            "6. **Release of Oxygen**: Oxygen gas is released into the atmosphere through respiration.\n",
            "\n",
            "7. **Storage of Glucose**: The glucose produced during photosynthesis is stored in the form of starch, which can be converted back to glucose for respiration in the dark.\n",
            "\n",
            "Photosynthesis is essential for life on Earth and is a vital process that sustains the food chain and supports life forms from plants to animals.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "class ContentError(Exception):\n",
        "    \"\"\"Custom exception for content fetching errors.\"\"\"\n",
        "    pass\n",
        "\n",
        "def fetch_wikipedia_content(wiki_url: str) -> str:\n",
        "    \"\"\"\n",
        "    Fetch and clean content from Wikipedia URL.\n",
        "\n",
        "    Args:\n",
        "        wiki_url: Wikipedia article URL\n",
        "\n",
        "    Returns:\n",
        "        str: Cleaned article text\n",
        "\n",
        "    Raises:\n",
        "        ContentError: If content cannot be fetched or processed\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Validate Wikipedia URL\n",
        "        if not re.match(r'https?://[a-z]+\\.wikipedia\\.org/wiki/', wiki_url):\n",
        "            raise ContentError(\"Invalid Wikipedia URL format\")\n",
        "\n",
        "        # Fetch the page\n",
        "        response = requests.get(wiki_url)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Remove unwanted sections\n",
        "        for unwanted in soup.find_all(['table', 'script', 'style', 'sup', 'span.mw-editsection']):\n",
        "            unwanted.decompose()\n",
        "\n",
        "        # Get the main content\n",
        "        content_div = soup.find(id='mw-content-text')\n",
        "        if not content_div:\n",
        "            raise ContentError(\"Could not find main content\")\n",
        "\n",
        "        # Extract paragraphs\n",
        "        paragraphs = content_div.find_all('p')\n",
        "\n",
        "        # Clean and join the text\n",
        "        content = ' '.join(\n",
        "            p.get_text().strip()\n",
        "            for p in paragraphs\n",
        "            if p.get_text().strip()  # Skip empty paragraphs\n",
        "        )\n",
        "\n",
        "        # Clean up special characters and extra whitespace\n",
        "        content = re.sub(r'\\[\\d+\\]', '', content)  # Remove reference numbers\n",
        "        content = re.sub(r'\\s+', ' ', content)  # Normalize whitespace\n",
        "\n",
        "        if not content:\n",
        "            raise ContentError(\"No content found in the article\")\n",
        "\n",
        "        return content\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        raise ContentError(f\"Failed to fetch Wikipedia page: {str(e)}\")\n",
        "    except Exception as e:\n",
        "        raise ContentError(f\"Error processing Wikipedia content: {str(e)}\")\n",
        "\n",
        "def preprocess_content(content: str, chunk_size: int = 200) -> List[str]:\n",
        "    \"\"\"\n",
        "    Preprocess content into chunks with improved text splitting.\n",
        "    Works for both Wikipedia and YouTube content.\n",
        "\n",
        "    Args:\n",
        "        content: Input text content\n",
        "        chunk_size: Maximum size of each chunk\n",
        "\n",
        "    Returns:\n",
        "        List of text chunks\n",
        "    \"\"\"\n",
        "    # Split on sentence boundaries\n",
        "    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', content) if s.strip()]\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_length = 0\n",
        "\n",
        "    for sentence in sentences:\n",
        "        sentence_length = len(sentence)\n",
        "        if current_length + sentence_length <= chunk_size:\n",
        "            current_chunk.append(sentence)\n",
        "            current_length += sentence_length\n",
        "        else:\n",
        "            if current_chunk:\n",
        "                chunks.append(' '.join(current_chunk))\n",
        "            current_chunk = [sentence]\n",
        "            current_length = sentence_length\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(' '.join(current_chunk))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def query_llama_model(api_key: str, prompt: str) -> Optional[str]:\n",
        "    \"\"\"Existing Groq API query function\"\"\"\n",
        "    if not api_key:\n",
        "        raise ValueError(\"API key is required\")\n",
        "\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {api_key}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "        \"model\": \"llama-3.1-8b-instant\",\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a helpful assistant that answers questions about content based on provided context.\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt\n",
        "            }\n",
        "        ],\n",
        "        \"temperature\": 0.7\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(\n",
        "            \"https://api.groq.com/openai/v1/chat/completions\",\n",
        "            headers=headers,\n",
        "            json=payload,\n",
        "            timeout=30\n",
        "        )\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            logging.error(f\"API Error: {response.status_code} - {response.text}\")\n",
        "            return None\n",
        "\n",
        "        result = response.json()\n",
        "        if not result.get(\"choices\"):\n",
        "            raise ValueError(\"No choices in response\")\n",
        "\n",
        "        return result[\"choices\"][0][\"message\"][\"content\"].strip()\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logging.error(f\"API request failed: {str(e)}\")\n",
        "        return None\n",
        "    except (KeyError, ValueError) as e:\n",
        "        logging.error(f\"Error parsing API response: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def content_qa_system(url: str, question: str, api_key: str) -> str:\n",
        "    \"\"\"\n",
        "    Main QA system that handles both Wikipedia and YouTube URLs.\n",
        "\n",
        "    Args:\n",
        "        url: Wikipedia or YouTube URL\n",
        "        question: User question\n",
        "        api_key: Groq API key\n",
        "\n",
        "    Returns:\n",
        "        Answer string or error message\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Input validation\n",
        "        if not url or not question or not api_key:\n",
        "            raise ValueError(\"Missing required parameters\")\n",
        "\n",
        "        # Determine content type and fetch accordingly\n",
        "        if 'wikipedia.org' in url:\n",
        "            content = fetch_wikipedia_content(url)\n",
        "            content_type = \"Wikipedia article\"\n",
        "        elif 'youtu' in url:  # Handles youtube.com and youtu.be\n",
        "            content = fetch_youtube_transcript(url)\n",
        "            content_type = \"video transcript\"\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported URL type. Please provide a Wikipedia or YouTube URL.\")\n",
        "\n",
        "        if not content:\n",
        "            return f\"Could not extract content from {content_type}\"\n",
        "\n",
        "        # Process content\n",
        "        chunks = preprocess_content(content)\n",
        "        if not chunks:\n",
        "            return \"Failed to process content\"\n",
        "\n",
        "        # Prepare prompt\n",
        "        context = \" \".join(chunks)\n",
        "        prompt = (\n",
        "            f\"Based on the following {content_type}, please answer the question.\\n\\n\"\n",
        "            f\"Content: {context}\\n\\n\"\n",
        "            f\"Question: {question}\\n\\n\"\n",
        "            f\"Answer:\"\n",
        "        )\n",
        "\n",
        "        # Get model response\n",
        "        answer = query_llama_model(api_key, prompt)\n",
        "        if not answer:\n",
        "            return \"Failed to generate answer\"\n",
        "\n",
        "        return answer\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in QA system: {str(e)}\")\n",
        "        return f\"An error occurred: {str(e)}\"\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    url = \"https://simple.wikipedia.org/wiki/Photosynthesis\"\n",
        "    question =\"Expain process of Photosynthesis?\"\n",
        "    groq_api_key = \"\"  # Replace with actual API key\n",
        "\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "    answer = content_qa_system(url, question, groq_api_key)\n",
        "    print(\"Answer:\", answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQDgPNEDXa8M"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
